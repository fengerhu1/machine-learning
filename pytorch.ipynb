{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03363afb17fd439d9e2263e2401253b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5b7f21aa7d42818c08815f053746e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3e532c34aa4ee286e57d90e386601b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab169f90ad0a450c9e2b88012ba3de89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleNet(nn.Module):\n",
    "# TODO:define model\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # 输入1通道，输出10通道，kernel 5*5\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.conv3 = nn.Conv2d(20, 40, 3)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        # fully connect\n",
    "        self.fc = nn.Linear(40, 10)#（in_features, out_features）\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in_size = 128\n",
    "        in_size = x.size(0) # one batch     此时的x是包含batchsize维度为4的tensor，即(batchsize，channels，x，y)，x.size(0)指batchsize的值    把batchsize的值作为网络的in_size\n",
    "        # x: 128*1*28*28\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        # x: 128*10*12*12  feature map =[(28-4)/2]^2=12*12\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        # x: 128*20*4*4\n",
    "        x = F.relu(self.mp(self.conv3(x)))\n",
    "\n",
    "        x = x.view(in_size, -1) # flatten the tensor 相当于resharp\n",
    "        # print(x.size())\n",
    "        # x: 128*320\n",
    "        x = self.fc(x)\n",
    "        # x:128*10\n",
    "        # print(x.size())\n",
    "        return F.log_softmax(x)  #128*10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()    \n",
    "model = SimpleNet()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengdahu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.315385\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.162656\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.096972\n",
      "\n",
      "Train set:  Accuracy: 57380/60000 (96%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengdahu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1100, Accuracy: 9647/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.082535\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.056804\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.107519\n",
      "\n",
      "Train set:  Accuracy: 57965/60000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0884, Accuracy: 9700/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.157537\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.078290\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.051071\n",
      "\n",
      "Train set:  Accuracy: 58298/60000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0778, Accuracy: 9740/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.093892\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.062354\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.125021\n",
      "\n",
      "Train set:  Accuracy: 58500/60000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0682, Accuracy: 9767/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.157255\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.085413\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.167450\n",
      "\n",
      "Train set:  Accuracy: 58635/60000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0635, Accuracy: 9777/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.078962\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.109917\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.087145\n",
      "\n",
      "Train set:  Accuracy: 58735/60000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0611, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.077916\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.094426\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.078092\n",
      "\n",
      "Train set:  Accuracy: 58820/60000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0553, Accuracy: 9800/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.035143\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.038370\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.014237\n",
      "\n",
      "Train set:  Accuracy: 58901/60000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0526, Accuracy: 9811/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.034744\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.021014\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.037482\n",
      "\n",
      "Train set:  Accuracy: 58981/60000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0501, Accuracy: 9809/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):#batch_idx是enumerate（）函数自带的索引，从0开始\n",
    "        # data.size():[128, 1, 28, 28]\n",
    "        # target.size():[128]\n",
    "\n",
    "        output = model(data)\n",
    "        #output:128*10\n",
    "\n",
    "        loss=criterion(output, target)\n",
    "        #loss = F.nll_loss(output, target)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100 * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "        optimizer.zero_grad()   # 所有参数的梯度清零\n",
    "        loss.backward()         #即反向传播求梯度\n",
    "        optimizer.step()        #调用optimizer进行梯度下降更新参数\n",
    "    print('\\nTrain set:  Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #print(pred)\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
